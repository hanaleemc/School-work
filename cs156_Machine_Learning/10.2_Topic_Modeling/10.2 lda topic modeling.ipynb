{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import requests\n",
    "import numpy as np\n",
    "import string\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/cache/epub/174/pg174.txt\"\n",
    "r = requests.get(url)\n",
    "# r.encoding = 'ISO-8859-1'\n",
    "# r.encoding = 'utf-8'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = r.text\n",
    "exclude = set(string.punctuation)\n",
    "raw = ''.join(ch for ch in raw if ch not in exclude)\n",
    "raw = raw.replace(\"\\r\", \" \")\n",
    "# # # raw.replace(\"\\u\", \" \")\n",
    "raw = raw.replace(\"\\n\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##option1- chapters\n",
    "\n",
    "# print (r.encoding )\n",
    "\n",
    "\n",
    "chapters = raw.split('CHAPTER')\n",
    "chapters= chapters[1:]\n",
    "\n",
    "sub_chaps = []\n",
    "\n",
    "for chapter in chapters:\n",
    "    l= len(chapter)\n",
    "    sub_chaps.append(chapter[:l])\n",
    "    sub_chaps.append(chapter[-l:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "##option 2 - par\n",
    "chapters = raw.split('CHAPTER')\n",
    "chapters= chapters[1:]\n",
    "prs = []\n",
    "for c in chapters:\n",
    "    par = c.split('    ')\n",
    "    for p in par:\n",
    "        if len(p)>10:\n",
    "            prs.append(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "Fitting LDA models with tf features, n_samples=1558 and n_features=300...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=150, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = prs\n",
    "n_samples = len(data)\n",
    "n_features = 300\n",
    "n_topics = 10\n",
    "n_top_words = 5\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples={} and n_features={}...\".format(n_samples, n_features)\n",
    "      )\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=150,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in LDA model:\n",
      "\n",
      "Topic 0: lord henry said mr gray\n",
      "Topic 1: harry dorian dont said cried\n",
      "Topic 2: project work gutenbergtm works electronic\n",
      "Topic 3: life world art day soul\n",
      "Topic 4: face women look duchess lady\n",
      "Topic 5: man round hand little men\n",
      "Topic 6: dont know want people like\n",
      "Topic 7: dorian gray head said come\n",
      "Topic 8: basil life picture portrait thought\n",
      "Topic 9: like room long eyes face\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics in LDA model:\\n\")\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    words = \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(\"Topic {}: {}\".format(topic_idx, words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-83168.7047306919"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.score(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print (np.sum(np.random.dirichlet([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
